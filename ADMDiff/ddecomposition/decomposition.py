import torch
from torch import nn

from ddecomposition.block import SpatialTemporalTransformerBlock, TemporalTransformerBlock, DecompositionBlock
from ddecomposition.embedding import DataEmbedding, PositionEmbedding, TimeEmbedding
from ddecomposition.subtraction import OffsetSubtraction
import torch.nn.functional as F


class DataEncoder(nn.Module):
    def __init__(self, window_size, model_dim, ff_dim, atten_dim, feature_num, block_num, head_num, dropout):
        super(DataEncoder, self).__init__()
        self.data_embedding = DataEmbedding(model_dim, feature_num)
        self.position_embedding = PositionEmbedding(model_dim)

        self.encoder_blocks = nn.ModuleList()
        for i in range(block_num):
            dp = 0 if i == block_num - 1 else dropout
            self.encoder_blocks.append(
                SpatialTemporalTransformerBlock(window_size, model_dim, ff_dim, atten_dim, head_num, dp)
            )

    def forward(self, x):
        x = self.data_embedding(x) + self.position_embedding(x)

        for block in self.encoder_blocks:
            x = block(x)

        return x


class TimeEncoder(nn.Module):
    def __init__(self, model_dim, ff_dim, atten_dim, time_num, block_num, head_num, dropout):
        super(TimeEncoder, self).__init__()
        self.time_embed = TimeEmbedding(model_dim, time_num)

        self.encoder_blocks = nn.ModuleList()
        for i in range(block_num):
            dp = 0 if i == block_num - 1 else dropout
            self.encoder_blocks.append(
                TemporalTransformerBlock(model_dim, ff_dim, atten_dim, head_num, dp)
            )

    def forward(self, x):
        x = self.time_embed(x)

        for block in self.encoder_blocks:
            x = block(x)

        return x





class DynamicDecomposition(nn.Module):
    def __init__(self, window_size, model_dim, ff_dim, atten_dim, feature_num,
                 time_num, block_num, head_num, dropout, d,
                 adaptive=False, tol=1e-3):
        super(DynamicDecomposition, self).__init__()
        self.data_encoder = DataEncoder(window_size, model_dim, ff_dim, atten_dim,
                                        feature_num, block_num, head_num, dropout)
        self.time_encoder = TimeEncoder(model_dim, ff_dim, atten_dim,
                                        time_num, block_num, head_num, dropout)

        self.decomposition_blocks = nn.ModuleList()
        for i in range(block_num):
            dp = 0 if i == block_num - 1 else dropout
            self.decomposition_blocks.append(
                DecompositionBlock(model_dim, ff_dim, atten_dim, feature_num, head_num, dp)
            )

        self.minus = OffsetSubtraction(window_size, feature_num, d)

        # ğŸ”§ æ–°å¢ï¼šè‡ªé€‚åº”æ§åˆ¶å‚æ•°
        self.adaptive = adaptive
        self.tol = tol
        self.max_blocks = block_num

    def forward(self, data, time):
        """
        data: [B, L, F]   åŸå§‹è¾“å…¥
        time: [B, L, T]   æ—¶é—´ç‰¹å¾
        return:
            stable: [B, L, F]  ç¨³å®šæˆåˆ†
            trend:  [B, L, F]  è¶‹åŠ¿æˆåˆ†
        """
        residual = data.clone()   # ä¿ç•™åŸå§‹è¾“å…¥
        data = self.data_encoder(data)   # [B, L, D]
        time = self.time_encoder(time)   # [B, L, D]

        stable = torch.zeros_like(residual).to(data.device)  # [B, L, F]

        prev_residual = residual
        for i, block in enumerate(self.decomposition_blocks):
            tmp_stable, data = block(data, time)   
            stable = stable + tmp_stable

            # ğŸ”§ æ›´æ–°æ®‹å·®
            residual = residual - tmp_stable

            # ğŸ”§ è‡ªé€‚åº”ç»ˆæ­¢ï¼šå¦‚æœæ®‹å·®æ–¹å·®è¶³å¤Ÿå°ï¼Œå°±æå‰åœæ­¢
            if self.adaptive:
                diff = torch.var(residual - prev_residual) / (torch.var(prev_residual) + 1e-8)
                if diff < self.tol:
                    break
                prev_residual = residual

        # è¶‹åŠ¿éƒ¨åˆ†
        trend = self.minus(residual, stable)   # [B, L, F]
        trend = torch.mean(trend, dim=1).unsqueeze(1).repeat(1, data.shape[1], 1)  # [B, L, F]

        return stable, trend


